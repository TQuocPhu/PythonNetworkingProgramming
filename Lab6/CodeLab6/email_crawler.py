import urllib.request
import re
from urllib.parse import urljoin, urlparse
from collections import deque

visited_urls = set()
found_emails = set()

def is_same_domain(base_url, target_url):
    return urlparse(base_url).netloc == urlparse(target_url).netloc

def crawl(start_url):
    queue = deque([start_url])

    while queue:
        url = queue.popleft()
        if url in visited_urls:
            continue

        print(f"\n ƒêang ki·ªÉm tra: {url}")
        visited_urls.add(url)

        try:
            with urllib.request.urlopen(url, timeout=5) as response:
                content_type = response.headers.get("Content-Type", "")
                if "text/html" not in content_type:
                    continue

                html = response.read().decode(errors='ignore')


                print("\n--- N·ªôi dung HTML ---")
                print(html[:500])  # In 500 k√Ω t·ª± ƒë·∫ßu ti√™n
                print("--- K·∫øt th√∫c tr√≠ch HTML ---\n")

                # T√¨m email
                emails = re.findall(r'[\w.+-]+@[\w-]+\.[\w.-]+', html)
                found_emails.update(emails)

                # T√¨m URL
                urls = re.findall(
                    r'https?://(?:[a-zA-Z0-9]|[$-_@.&+]|[!*(),]|(?:%[0-9a-fA-F]{2}))+',
                    html
                )

                for link in urls:
                    full_url = urljoin(url, link)
                    if is_same_domain(start_url, full_url) and full_url not in visited_urls:
                        queue.append(full_url)

        except Exception as e:
            print(f"L·ªói khi truy c·∫≠p {url}: {e}")

def main():
    start_url = input("Nh·∫≠p URL trang web (VD: https://example.com): ").strip()
    print("B·∫Øt ƒë·∫ßu t√¨m ki·∫øm email...\n")
    crawl(start_url)

    print("\nHo√†n t·∫•t. C√°c email t√¨m th·∫•y:")
    if found_emails:
        for email in sorted(found_emails):
            print("üìß", email)
    else:
        print("Kh√¥ng t√¨m th·∫•y email n√†o.")

if __name__ == "__main__":
    main()